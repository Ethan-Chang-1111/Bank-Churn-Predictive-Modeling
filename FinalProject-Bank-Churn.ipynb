{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "MPiQLMfIWGjn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbRgopzfVyFc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import seaborn as sn\n",
        "\n",
        "# ANN - Ethan\n",
        "# SVM - Lily\n",
        "# RF - Marinana\n",
        "# Adaboost - Qingru\n",
        "\n",
        "# Source: https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn/data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BvUDC9_iZ57U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "\n",
        "# splice and reform data between categorical, dummy categorical, and numeric\n",
        "def spliceData(cleandata_df, categorical_columns, dummy_categorical_columns, numeric_columns, target_columns):\n",
        "    # splice out columns\n",
        "    categorical_df = cleandata_df[categorical_columns]\n",
        "    dummy_categorical_df = cleandata_df[dummy_categorical_columns]\n",
        "    numeric_df = cleandata_df[numeric_columns]\n",
        "    ##TODO: Scale numerical data\n",
        "    # Make dummies\n",
        "    dummy_code_cat_var = pd.get_dummies(dummy_categorical_df, drop_first=False, dtype='uint8')\n",
        "\n",
        "    df_predictor = pd.concat([categorical_df, numeric_df, dummy_code_cat_var], axis=1)\n",
        "    target = cleandata_df[target_columns]\n",
        "    return target, df_predictor\n",
        "\n",
        "\n",
        "# Split the data into test and train\n",
        "from sklearn.model_selection import train_test_split\n",
        "def splitData(predictor, target, size):\n",
        "    x_train, x_test, y_train, y_test = train_test_split(predictor, target, test_size=size)\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Show Classification, Confusion Matrix, and ROC AUC\n",
        "def display_triad(model, x_test_scale, y_test, matrix_title):\n",
        "    y_pred = model.predict(x_test_scale)\n",
        "    display_classification_report(y_test, y_pred)\n",
        "\n",
        "    display_confusion_matrix(y_test, y_pred, matrix_title)\n",
        "\n",
        "    y_prob_pred = model.predict_proba(x_test_scale)[:,1]\n",
        "    display_auc_roc(y_test, y_prob_pred)\n",
        "\n",
        "# Show ROC AUC curve\n",
        "from sklearn import metrics\n",
        "def display_auc_roc(y_test, prob_pred):\n",
        "    fpr, tpr, t = metrics.roc_curve(y_test, prob_pred)\n",
        "    auc = metrics.roc_auc_score(y_test, prob_pred)\n",
        "    plt.plot(fpr, tpr, label = \"AUC=\"+str(100*round(auc,3))+\"%\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Show Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "def display_classification_report(y_test, y_pred):\n",
        "    # Predict a value from the model\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Show Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def display_confusion_matrix(y_test, y_pred, title):\n",
        "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    # cnf_matrix\n",
        "    cf_df = pd.DataFrame(cnf_matrix, columns=['0', '1'], index=['0', '1'])\n",
        "    plt.figure(figsize=(7,5))\n",
        "    sn.heatmap(cf_df, annot=True, fmt='g')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predictor')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nJFtg7EsWD5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can we determine if a customer will leave the bank, so that the bank can offer new promotions to keep that customer?\n",
        "# Since its more expensive to sign on a new customer than to keep\n",
        "\n",
        "churn = pd.read_csv(\"Customer-Churn-Records.csv\")\n",
        "churn.columns\n",
        "\n",
        "# Take out 'RowNumber', 'CustomerId', 'Surname' - Useless\n",
        "# take out 'Complain', 'Satisfaction Score' - Endogenous"
      ],
      "metadata": {
        "id": "XyMYxnDUWI94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate churn rate for each age group\n",
        "churn['Age_Group'] = pd.cut(churn['Age'], bins=range(0, 101, 10), right=False)\n",
        "\n",
        "# Calculate churn rate for each age group\n",
        "churn_rate_per_age_group = churn.groupby('Age_Group')['Exited'].mean()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "churn_rate_per_age_group.plot(kind='bar')\n",
        "plt.title('Churn Rate by Age Group',fontsize = 16)\n",
        "plt.xlabel('Age Group',fontsize = 14)\n",
        "plt.ylabel('Churn Rate',fontsize = 14)\n",
        "plt.xticks(rotation=45,fontsize = 12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zo-kez2pU9oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Geography', hue='Exited', data=churn)\n",
        "plt.title('Churn by Geography')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UCINXhP9bPRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Create a pie chart of the churn distribution\n",
        "labels = [\"Not Exited\", \"Exited\"]\n",
        "sizes = [len(churn[churn['Exited'] == 0]), len(churn[churn['Exited'] == 1])]\n",
        "colors = [\"green\", \"lightcoral\"]\n",
        "\n",
        "plt.pie(sizes, labels=labels, autopct=\"%1.1f%%\", colors=colors)\n",
        "plt.title(\"Churn Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3L_3zMRfbS4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning"
      ],
      "metadata": {
        "id": "a0AjxDRkWLp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  'Geography', 'Card Type','Gender',\n",
        "cols = ['CreditScore',\n",
        "       'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
        "       'IsActiveMember', 'EstimatedSalary', 'Exited', 'Point Earned']\n",
        "\n",
        "\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(10, 6))  # Set the figure size\n",
        "sns.heatmap(churn[cols].corr(), annot=True)  # cmap is the color map\n",
        "plt.title('Heatmap of DataFrame')  # Set title\n",
        "# plt.xlabel('Columns')  # Set x-axis label\n",
        "# plt.ylabel('Index')  # Set y-axis label\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mU1R_vHdWLR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps, normalize, scale, and clean data.\n",
        "# Class balance, exists an 80/20 split in class\n",
        "# Incredibly, there are no missing values at all.\n",
        "using = ['CreditScore',\n",
        "       'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
        "       'Geography', 'Card Type', 'Gender',\n",
        "       'IsActiveMember', 'EstimatedSalary', 'Exited', 'Point Earned']\n",
        "churn[using].info()"
      ],
      "metadata": {
        "id": "kB0fJoTrWN_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummify and Scale\n",
        "cat_list = []\n",
        "numeric_list = ['CreditScore',\n",
        "       'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
        "       'IsActiveMember', 'EstimatedSalary', 'Point Earned']\n",
        "dummy_list = ['Geography', 'Card Type', 'Gender', ]\n",
        "target_list = ['Exited']\n",
        "\n",
        "target_df, predictor_df = spliceData(churn, cat_list, dummy_list, numeric_list, target_list)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(predictor_df[numeric_list], target_df)\n",
        "predictor_df_scale = pd.DataFrame(scaler.transform(predictor_df[numeric_list]))\n",
        "\n",
        "predictor_df_scale.columns = predictor_df[numeric_list].columns.values\n",
        "predictor_df_scale.index = predictor_df[numeric_list].index.values\n",
        "\n",
        "predictor_df[numeric_list] = predictor_df_scale"
      ],
      "metadata": {
        "id": "N9cAuzu3nA0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class Balancing\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=1000)\n",
        "\n",
        "predictor_df, target_df = smote.fit_resample(predictor_df, target_df)"
      ],
      "metadata": {
        "id": "uBwG3CastUGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Test Split\n",
        "x_train, x_test, y_train, y_test = splitData(predictor_df, target_df, size=0.3)"
      ],
      "metadata": {
        "id": "fMutiYq1tVCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "labels = [\"Not Exited\", \"Exited\"]\n",
        "sizes_train = [len(y_train[y_train['Exited'] == 0]), len(y_train[y_train['Exited'] == 1])]\n",
        "sizes_test = [len(y_test[y_test['Exited'] == 0]), len(y_test[y_test['Exited'] == 1])]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "\n",
        "axs[0].pie(sizes_train, labels=labels, autopct=\"%1.1f%%\", colors=[\"green\", \"lightcoral\"])\n",
        "axs[0].set_title(\"Train Set\")\n",
        "\n",
        "axs[1].pie(sizes_test, labels=labels, autopct=\"%1.1f%%\", colors=[\"green\", \"lightcoral\"])\n",
        "axs[1].set_title(\"Test Set\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fTDRyTK8bXRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANN"
      ],
      "metadata": {
        "id": "Q7Xn8N6ImTXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup ANN\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import regularizers\n",
        "\n",
        "# Ann - Basic\n",
        "print(x_train.shape)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(20,input_shape=(18,),activation=\"sigmoid\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(20, activation=\"selu\", kernel_regularizer=regularizers.l2()))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(20, activation=\"selu\", kernel_regularizer=regularizers.l2()))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(20, activation=\"selu\", kernel_regularizer=regularizers.l2()))\n",
        "model.add(Dropout(0.2))\n",
        "# model.add(Dense(18,activation=\"selu\", kernel_regularizer=regularizers.l2()))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "model.compile(optimizer=Adam(learning_rate=0.01),loss='binary_crossentropy',metrics = ['accuracy'])\n",
        "ann = model.fit(x=x_train, y=y_train, verbose=0, epochs=100, callbacks=[early_stop], validation_split=0.3)"
      ],
      "metadata": {
        "id": "NiSDzKOSmTGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict prob of y and classes\n",
        "y_pred_proba = model.predict(x_test)\n",
        "y_pred = [1 if x>0.5 else 0 for x in y_pred_proba]\n",
        "\n",
        "# Calculate classification metrics\n",
        "print(y_pred_proba)\n",
        "print(y_pred)\n",
        "display_auc_roc(y_test, y_pred_proba)\n",
        "display_confusion_matrix(y_test, y_pred, \"ANN\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "p0AM0LZs_qpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM"
      ],
      "metadata": {
        "id": "SGP2_qGZmWsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " SVM\n",
        "svm_model = SVC(probability=True)\n",
        "svm_model.fit(x_train, y_train)\n",
        "display_triad(svm_model, x_test, y_test, \"SVM Confusion Matrix\")\n",
        "\n",
        "# Hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'gamma': [0.01, 0.1, 1, 10, 100], 'C': [0.01, 0.1, 1, 10, 100], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\n",
        "grid.fit(x_train, y_train)\n",
        "print(grid.best_params_)\n",
        "\n",
        "svm_model_tuned = SVC(**grid.best_params_)\n",
        "svm_model_tuned.fit(x_train, y_train)\n",
        "display_triad(svm_model_tuned, x_test, y_test, \"SVM Tuned Confusion Matrix\")"
      ],
      "metadata": {
        "id": "WCMFURgtmYN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RF"
      ],
      "metadata": {
        "id": "ie6T38iDmYrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create a parameter grid\n",
        "params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=params, cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(best_score)\n",
        "print(best_params)"
      ],
      "metadata": {
        "id": "j7ZJyZXqmaPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = RandomForestClassifier(**best_params)\n",
        "model_RF = classifier.fit(x_train,y_train)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = model_RF.predict(x_test)\n",
        "display_triad(model_RF, x_test, y_test, \"Random Forest Classifier\")"
      ],
      "metadata": {
        "id": "FOsV54YOkx1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances = pd.Series(classifier.feature_importances_ , index = x_train.columns)\n",
        "feature_importances.nlargest(10).plot(kind='barh')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p0xB9w2gk0MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "model_filename = \"RandomForest.model\"\n",
        "pickle.dump(model_RF, open(model_filename,'wb'))\n",
        "\n",
        "from google.colab import files\n",
        "files.download('RandomForest.model')"
      ],
      "metadata": {
        "id": "JsPWeaZhlmZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adaboost"
      ],
      "metadata": {
        "id": "j-IZN3FNmanO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HP tuning example for AdaBoost\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "adb_classifier = AdaBoostClassifier()\n",
        "params = {'learning_rate':[0.2, 0.5, 1.0, 1.5],\n",
        "          'n_estimators': [25, 50, 100, 150, 200],\n",
        "          'random_state': [0,2,4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=adb_classifier, param_grid=params, cv=5, return_train_score = True)\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# show the best parameters\n",
        "best_adb_params = grid_search.best_params_"
      ],
      "metadata": {
        "id": "jh5vTFIRmcNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_adb_params"
      ],
      "metadata": {
        "id": "OJrS7i4stPf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "AdaBoost = AdaBoostClassifier(**best_adb_params)\n",
        "\n",
        "model_AB = AdaBoost.fit(x_train, y_train)\n",
        "y_pred = model_AB.predict(x_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "azyniq-gtUeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_triad(model_AB, x_test, y_test, 'AdaBoost Classifier Matrix')"
      ],
      "metadata": {
        "id": "aqOmx_TmtX9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}